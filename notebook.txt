我现在想做一个关于煤层气的排水采气的强化学习代理。
这个煤层气有5口井，对于生产过程的36个时间步，每个时间步我都会进行排水，排水的值有变大10%、不变、变小10%3个值，每个时间步改变排水后就会有一个产气量，这个产气量属于环境接收到排水量输出的，输入到输出由一个神经网络来提供，每次输出考虑前三个以及当前时间步作为输入，也就是说

我现在有一个神经网络可以作为环境，神经网络的输入是四行六列的矩阵，当前时间为t，则输入为[[时间步t-3,井1排水量，..,..,..,井5排水量],[时间步t-2,井1排水量，..,..,..,井5排水量],[时间步t-1,井1排水量，..,..,..,井5排水量],[时间步t,井1排水量，..,..,..,井5排水量]]。
神经网络的输出为一行5列的矩阵，为当前时刻t下的[井1产气量，井2产气量，井3产气量，井4产气量，井5产气量]怎么适配这个强化学习代码呢。
环境所使用的神经网络网络可以通过：
from models.cnn_lstm import CNNLSTM
input_size = 6
hidden_size = 30
num_layers = 3
output_size = 5
model = CNNLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)
# model.to(device)
model.load_state_dict(torch.load(''.\weight\cnn_lstm_model_parameters_2024.11.21.pth'))
model.to(device)
model.eval()  # 设置为评估模式，避免训练时的 dropout 或 batch normalization 行为
print("Model parameters have been loaded successfully.")导入进来
神经网络的输入输出的值归一化可以用两个pkl文件：
scaler_input = joblib.load(''.\weight\scaler_input.pkl')
scaler_output = joblib.load(''.\weight\scaler_output.pkl')

我想通过强化学习来学习如何改变排水的量，我想的是用DQN算法，目前的思路：
一、强化学习方法
考虑到问题涉及到离散的动作空间（排水值有变大 10%、不变、变小 10% 这 3 种情况）以及连续多个时间步的决策过程，Q-Learning 或者基于深度 Q 网络（DQN）及其扩展（如 Double DQN、Dueling DQN 等）的方法比较合适。
Q-Learning 是一种经典的无模型强化学习算法，适用于离散动作空间的情况，能通过不断迭代学习来估计每个状态 - 动作对对应的价值（Q 值）。如果状态空间较为复杂，简单的 Q-Learning 可能难以处理，此时可以采用 DQN 及其变体，利用神经网络来近似 Q 值函数，更好地应对高维状态空间。

二、定义状态（State）
状态的内容
当前各井的排水量：包含 5 口井在当前时间步的实际排水量数值，这能直观反映当前排水的基本情况。
各井前三个时间步的排水量变化趋势：对于每口井，用编码来表示其前三个时间步排水量是上升、下降还是不变的情况。例如，0 表示下降，1 表示不变，2 表示上升。这样每口井就有 3 个时间步的趋势信息，总共 5 口井，这部分信息维度为 5×3。
当前的产气量数值：当前时间步整体所产生的气量数值，作为反映当前排水策略效果的关键指标。
状态的形状：
综合上述信息，假设每口井排水量用一个浮点数表示，产气量也用一个浮点数表示。那么状态可以表示为一个向量，其形状为 (5 + 5×3 + 1)，即 (21,) 的形状。其中 “5” 对应 5 口井当前的排水量，“5×3” 对应 5 口井前三个时间步的排水量变化趋势编码，“1” 对应当前的产气量数值。

三、定义动作（Action）
动作的内容
对于每口井，排水改变仍有 3 种情况（变大 10%、不变、变小 10%），可以用离散的整数值来表示动作，如 0 表示排水变小 10%，1 表示不变，2 表示排水变大 10%。那么在每个时间步，动作就是一个包含 5 个元素的向量，每个元素分别对应 5 口井中每一口井的排水改变动作。
动作的形状：
动作的形状为 (5,)，其中每个元素取值范围是 {0, 1, 2}，分别对应上述提到的 3 种排水改变情况。

四、定义价值（Value）
价值的含义
价值（用 Q 值表示）代表在特定状态下采取特定动作后，未来预期能够累积获得的奖励（此处产气量等相关因素可关联到奖励）大小的估计。智能体通过学习不断更新 Q 值，以此来判断在不同状态下选择何种动作更有利于长期最大化产气量。
价值的形状：
在使用深度 Q 网络等方法时，Q 值通常呈现为一个三维张量。其形状可以表示为 (状态数量, 动作数量, 1)。假设经过离散化等处理后，状态总共有 S 种可能情况，而动作有 3^5（因为每口井 3 种选择，5 口井组合起来的所有可能动作情况）种组合情况，那么 Q 值的形状就是 (S, 3^5, 1)。在简单的 Q-Learning 表格形式下，一般用二维表格存储，形状为 (状态数量, 动作数量)，每个元素对应一个状态 - 动作对的 Q 值。


上一次通过和你的交流，我现在有以下代码：
现在的问题是：1、我不知道代码是否有逻辑上的问题 2、我训练起来奖励并没有什么显著的变化 3、我不了解dqn强化学习所以思路不是太懂
import numpy as np
import torch
import gym
from gym import spaces
from models.cnn_lstm import CNNLSTM
import joblib
import warnings
# 忽略特定的 UserWarning
warnings.filterwarnings("ignore", category=UserWarning, message="X does not have valid feature names")
scaler_input = joblib.load(''.\weight\scaler_input.pkl')
scaler_output = joblib.load(''.\weight\scaler_output.pkl')

class GasExtractionEnv(gym.Env):
    def __init__(self, model):
        super(GasExtractionEnv, self).__init__()
        
        self.model = model  # 你的神经网络模型
        self.time_steps = 36  # 总共的时间步
        self.current_time = 0  # 当前时间步
        
        # 动作空间：3的5次方，即每个井分别有3个动作选择
        self.action_space = spaces.Discrete(3**5)  # 总共3^5=243种动作
        
        # 状态空间：4行6列矩阵（每个时间步的排水量）
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4, 6), dtype=np.float32)
        
        # 初始化状态
        self.state = np.zeros((4, 6))  # 4个时间步，每个时间步6个数据（5个井排水量 + 时间步）
        # 加载缩放器

        

    def reset(self):
        """重置环境"""
        # self.current_time = 0
        # time_step = np.array([self.current_time + i + 1 for i in range(4)])  # 时间步 t+1到t+4

        self.current_time = np.random.randint(3, self.time_steps)  # 随机选择一个时间步 t（大于等于3）
        #  初始化时间步：从 t-3 到 t
        time_step = np.array([self.current_time - 3 + i for i in range(4)])  # 时间步 t-3 到 t，确保时间步从1开始

        # 初始化每个井的排水量，假设我们使用蒙特卡洛随机分布生成10到50之间的值
        drainage = np.random.uniform(10, 50, size=(4, 5))  # 生成4个时间步，每个时间步5个井的排水量

        # 将时间步和排水量组合在一起
        self.state[:, 0] = time_step  # 第一列是时间步
        self.state[:, 1:] = drainage  # 剩余列是井的排水量

        # 使用 scaler_input 对状态进行归一化
        self.state = scaler_input.transform(self.state)  # 归一化处理

        return self.state

    
    def step(self, action):
        """执行动作并返回下一个状态和奖励"""
        
        # 记录当前状态（执行动作前的产气量）
        gas_before_scaler = self.model(torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).to(device))  # 输出为5个井的产气量
        # gas_before_true = scaler_output.inverse_transform(gas_before_scaler.cpu().detach().numpy())

        reward_before = np.sum(gas_before_scaler.cpu().detach().numpy())/5  # 当前时刻平均产气量作为奖励

        # 将action转换为每个井的排水调整量
        action_vector, action_explanation = self._decode_action(action)

        # 使用 scaler_input 对状态进行反归一化    
        self.state = scaler_input.inverse_transform(self.state) 

        # 更新状态：排水量变化，时间步不变
        for i in range(5):
            self.state[-1, i+1] *= (1 + action_vector[i])  # 只改变当前时间步的排水量

        # 使用 scaler_input 对状态进行归一化
        self.state = scaler_input.transform(self.state.reshape(-1, 6)).reshape(4, 6)  # 归一化处理

        # 计算执行动作后的产气量
        gas_after_scaler = self.model(torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).to(device))  # 输出为5个井的产气量
        # gas_after_true = scaler_output.inverse_transform(gas_after_scaler.cpu().detach().numpy())
        reward_after = np.sum(gas_after_scaler.cpu().detach().numpy())/5  # 当前时刻产气量平均值作为奖励

        # 奖励是产气量变化的差值
        reward = (reward_after - reward_before)*10

        # 更新当前时间
        self.current_time += 1

        done = self.current_time > self.time_steps  # 如果时间步数超过了限制，结束
        return self.state, reward, done, {'action_explanation': action_explanation}

    
    def render(self):
        """渲染环境（可选，方便调试）"""
        print(f"Time step {self.current_time}")
        print(f"Current state:\n{self.state}")
    
    def _decode_action(self, action):
        """
        将动作从整数转化为每个井的排水调整量。
        例如，action = 8 -> action_vector = [0.1, 0.0, -0.1, 0.1, -0.1]
        返回动作向量和对应的解释。
        """
        action_vector = []
        action_explanation = []
        for i in range(5):
            adjustment = (action // (3 ** i)) % 3
            if adjustment == 0:
                action_vector.append(0.1)   # 增加 10%
                action_explanation.append(f"井{i+1}: 增加 10%")
            elif adjustment == 1:
                action_vector.append(0.0)   # 不变
                action_explanation.append(f"井{i+1}: 不变")
            else:
                action_vector.append(-0.1)  # 减少 10%
                action_explanation.append(f"井{i+1}: 减少 10%")
        
        return action_vector, action_explanation
    


# 模拟你已有的CNN-LSTM模型
input_size = 6
hidden_size = 30
num_layers = 3
output_size = 5
model = CNNLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)

# 模拟加载的模型参数
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.load_state_dict(torch.load(''.\weight\cnn_lstm_model_parameters_2024.12.6.pth'))
model.to(device)
model.eval()  # 设置为评估模式，避免训练时的 dropout 或 batch normalization 行为
print("Model parameters have been loaded successfully.")



# 创建环境
env = GasExtractionEnv(model)

# 测试环境
state = env.reset()
print("Initial state:\n", state)

# 让我们尝试执行一个动作，并查看结果
action = env.action_space.sample()  # 随机选择一个动作
print(f"Selected action: {action}")

# 执行动作
next_state, reward, done, info = env.step(action)

# 输出动作解释
print("Next state:\n", next_state)
print("Reward:", reward)
print("Done:", done)
print("Action explanation:", info['action_explanation'])  # 动作解释

#环境已经ok






import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
from stable_baselines3.common.vec_env import DummyVecEnv

# 定义 Q 网络（DQN 的核心网络）
class QNetwork(nn.Module):
    def __init__(self, state_size, hidden_size, action_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, action_size)

    def forward(self, state):
        x = torch.relu(self.fc1(state))
        return self.fc2(x)

# 定义 DQN 代理
class DQNAgent:
    def __init__(self, state_size, action_size, model):
        self.state_size = state_size
        self.action_size = action_size
        self.model = model
        self.target_model = QNetwork(state_size, 64, action_size)  # 目标网络
        self.optimizer = optim.Adam(self.model.parameters())
        self.memory = deque(maxlen=2000)
        self.batch_size = 32
        self.epsilon = 1.0  # 探索率
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.gamma = 0.99  # 折扣因子

        # 将目标网络初始化为与当前模型相同
        self.update_target_model()

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)  # 探索
        state = torch.tensor(state, dtype=torch.float32)
        q_values = self.model(state)
        return torch.argmax(q_values).item()  # 利用

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))


    def replay(self):
        if len(self.memory) < self.batch_size:
            return
        batch = random.sample(self.memory, self.batch_size)
        for state, action, reward, next_state, done in batch:
            state = torch.tensor(state, dtype=torch.float32)
            next_state = torch.tensor(next_state, dtype=torch.float32)
            reward = torch.tensor(reward, dtype=torch.float32)
            done = torch.tensor(done, dtype=torch.float32)  # 将 done 转换为 float

            # 计算目标值
            target = reward + (1 - done) * self.gamma * torch.max(self.target_model(next_state))
            target_f = self.model(state)
            target_f[action] = target

            # 计算损失并进行反向传播
            loss = nn.MSELoss()(target_f, self.model(state))
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay


    def update_target_model(self):
        self.target_model.load_state_dict(self.model.state_dict())

# 创建环境（GasExtractionEnv）
env = GasExtractionEnv(model)

# 获取状态和动作空间的维度
state_size = np.prod(env.observation_space.shape)  # 将 4x6 状态展平为一维
action_size = env.action_space.n  # 动作空间大小（243）

# 初始化 DQN 代理
agent = DQNAgent(state_size, action_size, QNetwork(state_size, 64, action_size))  # 定义代理



# 训练过程
episodes = 1000
for e in range(episodes):
    state = env.reset()
    state = state.flatten()  # 展平状态为一维向量
    total_reward = 0
    done = False
    
    while not done:
        # 选择动作
        action = agent.act(state)
        
        # 执行动作并得到反馈
        next_state, reward, done, info = env.step(action)
        next_state = next_state.flatten()  # 展平状态
        
        # 记住经验
        agent.remember(state, action, reward, next_state, done)
        
        # 更新状态
        state = next_state
        total_reward += reward
        
        # 训练
        agent.replay()
    
    # 每隔一定步数更新目标网络
    if e % 10 == 0:
        agent.update_target_model()
    
    print(f"Episode {e+1}/{episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}")

# 保存模型
torch.save(agent.model.state_dict(), "'.\weight\dqn_gas_extraction_model.pth")

















我现在想做一个关于煤层气的排水采气的强化学习代理。
这个煤层气有5口井，对于生产过程的36个时间步，每个时间步我都会进行排水，排水的值有变大10%、不变、变小10%3个值，每个时间步改变排水后就会有一个产气量，这个产气量属于环境接收到排水量输出的，输入到输出由一个神经网络来提供，每次输出考虑前三个以及当前时间步作为输入，也就是说

我现在有一个神经网络可以作为环境，神经网络的输入是四行六列的矩阵，当前时间为t，则输入为[[时间步t-3,井1排水量，..,..,..,井5排水量],[时间步t-2,井1排水量，..,..,..,井5排水量],[时间步t-1,井1排水量，..,..,..,井5排水量],[时间步t,井1排水量，..,..,..,井5排水量]]。
神经网络的输出为一行5列的矩阵，为当前时刻t下的[井1产气量，井2产气量，井3产气量，井4产气量，井5产气量]怎么适配这个强化学习代码呢。
环境所使用的神经网络网络可以通过：
from models.cnn_lstm import CNNLSTM
input_size = 6
hidden_size = 30
num_layers = 3
output_size = 5
model = CNNLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)
# model.to(device)
model.load_state_dict(torch.load(''.\weight\cnn_lstm_model_parameters_2024.11.21.pth'))
model.to(device)
model.eval()  # 设置为评估模式，避免训练时的 dropout 或 batch normalization 行为
print("Model parameters have been loaded successfully.")导入进来
神经网络的输入输出的值归一化可以用两个pkl文件：
scaler_input = joblib.load(''.\weight\scaler_input.pkl')
scaler_output = joblib.load(''.\weight\scaler_output.pkl')

我想通过强化学习来学习如何改变排水的量，我想的是用DQN算法，目前有以下代码：
import numpy as np
import torch
import gym
from gym import spaces
from models.cnn_lstm import CNNLSTM
import joblib
import warnings
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore", category=UserWarning, message="X does not have valid feature names")

scaler_input = joblib.load(''.\weight\scaler_input.pkl')
scaler_output = joblib.load(''.\weight\scaler_output.pkl')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class GasExtractionEnv(gym.Env):
    """
    煤层气排采强化学习环境
    """
    def __init__(self, model):
        super(GasExtractionEnv, self).__init__()
        self.model = model
        self.time_steps = 36  # 总时间步数
        self.current_time = 0  # 当前时间步
        self.action_space = spaces.Discrete(3 ** 5)  # 动作空间，3的5次方表示每个井口有3种操作
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4, 6), dtype=np.float32)  # 状态空间
        self.state = np.zeros((4, 6))  # 初始化状态

    def reset(self):
        """
        重置环境状态
        """
        self.current_time = 0  # 重置时间步
        time_step = np.array([self.current_time + i + 1 for i in range(4)])  # 时间序列
        drainage = np.random.uniform(10, 50, size=(4, 5))  # 随机生成排采量
        self.state[:, 0] = time_step
        self.state[:, 1:] = drainage
        self.state = scaler_input.transform(self.state)  # 对状态进行归一化
        return self.state

    def step(self, action):
        """
        执行动作并返回新的状态、奖励、是否终止和其他信息
        """
        # 计算动作执行前的产气量
        gas_before = self.model(torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).to(device))
        gas_before = gas_before.cpu().detach().numpy().flatten()

        # 解码动作,排水量
        action_vector, action_explanation = self._decode_action(action)
        self.state = scaler_input.inverse_transform(self.state)

        # 更新状态，调整排采量
        for i in range(5):
            self.state[-1, i + 1] *= (1 + action_vector[i])

        # 重新归一化状态
        self.state = scaler_input.transform(self.state.reshape(-1, 6)).reshape(4, 6)

        # 计算动作执行后的产气量
        gas_after = self.model(torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).to(device))
        gas_after = gas_after.cpu().detach().numpy().flatten()

        # 当前排采成本
        current_drainage = self.state[-1, 1:]
        cost = sum(abs(action_vector[i] * current_drainage[i]) * 0.1 for i in range(5))
        cost = max(1e-6, min(cost, 1000))

        # 三种奖励函数尝试
        # 1. 产气量变化奖励
        reward_1 = sum(gas_after - gas_before)

        # 2. 排采与产气的加权奖励
        gas_benefit = sum(max(-1000, min(gas_after[i] - gas_before[i], 1000)) for i in range(5))
        normalized_cost = cost / 1000
        reward_2 = 50* -normalized_cost + 1.0 * gas_benefit / 1000

        # 3. 改变排采量的微小惩罚
        change_penalty = sum(0.01 for change in action_vector if change != 0)
        reward_3 = gas_benefit - normalized_cost - change_penalty

        # 选择奖励方案（可切换）
        reward = reward_1  # 或 reward_2 或 reward_3
        # reward = max(-100, min(reward, 100))  # 奖励值限制在[-100, 100]

        # 更新时间步
        self.current_time += 1
        done = self.current_time > self.time_steps  # 判断是否达到结束条件

        return self.state, reward, done, {'action_explanation': action_explanation}

    def _decode_action(self, action):
        """
        将动作解码为每口井的操作调整比例
        """
        action_decoded = np.zeros(5)
        explanation = []
        for i in range(5):
            action_decoded[i] = [-0.1, 0, 0.1][action % 3]  # 动作解码为-0.1, 0, 0.1
            explanation.append(f"Well {i+1}: {'Decrease' if action_decoded[i] < 0 else 'Increase' if action_decoded[i] > 0 else 'No Change'}")
            action //= 3
        return action_decoded, explanation

    def render(self):
        print(f"Time step {self.current_time}")
        print(f"Current state: {self.state}")


class QNetwork(nn.Module):
    def __init__(self, state_size, hidden_size, action_size):
        """
        初始化Q网络，用于估计状态动作价值（Q值）。

        参数:
        - state_size: 状态空间的大小（输入层节点数）
        - hidden_size: 隐藏层的节点数量
        - action_size: 动作空间的大小（输出层节点数）
        """
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, action_size)

    def forward(self, state):
        """
        前向传播函数，定义数据在网络中的流动过程。

        参数:
        - state: 输入的状态张量

        返回:
        - 经过网络计算后的Q值张量，形状为(1, action_size)，表示对应各个动作的Q值估计
        """
        x = torch.relu(self.fc1(state))
        return self.fc2(x)


class DQNAgent:
    def __init__(self, state_size, action_size, model):
        """
        初始化DQN智能体。

        参数:
        - state_size: 状态空间的大小
        - action_size: 动作空间的大小
        - model: 智能体使用的Q网络模型实例
        """
        self.state_size = state_size
        self.action_size = action_size
        self.model = model
        self.target_model = QNetwork(state_size, 64, action_size)  # 目标网络，用于稳定训练过程
        self.optimizer = optim.Adam(self.model.parameters())  # 使用Adam优化器更新网络参数
        self.memory = deque(maxlen=5000)  # 经验回放记忆，存储历史经验，最大长度为2000
        self.batch_size = 32  # 每次从记忆中采样进行训练的批量大小
        self.epsilon = 1.0  # 初始探索率，用于平衡探索新动作和利用已知最优动作
        self.epsilon_min = 0.01  # 探索率的最小值，避免探索率降为0导致完全停止探索
        self.epsilon_decay = 0.9998  # 探索率的衰减率，控制探索率随训练轮数下降的速度
        self.gamma = 0.9  # 折扣因子，用于衡量未来奖励对当前价值的影响

        # 将目标网络初始化为与当前模型相同的参数
        self.update_target_model()  

    def act(self, state):
        """
        根据当前状态选择一个动作，基于探索率决定是探索新动作还是利用已知最优动作。

        参数:
        - state: 当前环境状态

        返回:
        - 选择的动作编号，是一个整数
        """
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)  # 探索：随机选择一个动作
        state = torch.tensor(state, dtype=torch.float32)
        q_values = self.model(state)
        return torch.argmax(q_values).item()  # 利用：选择Q值最大的动作

    def remember(self, state, action, reward, next_state, done):
        """
        将当前的状态、动作、奖励、下一个状态以及是否结束的信息添加到经验回放记忆中。

        参数:
        - state: 当前状态
        - action: 执行的动作
        - reward: 获得的奖励
        - next_state: 执行动作后的下一个状态
        - done: 是否结束的标志
        """
        self.memory.append((state, action, reward, next_state, done))

    def replay(self):
        """
        从经验回放记忆中采样一批数据进行训练，更新Q网络的参数。
        """
        if len(self.memory) < self.batch_size:
            return
        batch = random.sample(self.memory, self.batch_size)
        for state, action, reward, next_state, done in batch:
            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)
            reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)
            done = torch.tensor(done, dtype=torch.float32).unsqueeze(0)

            # 计算目标Q值，考虑即时奖励和未来折扣奖励
            target = reward + (1 - done) * self.gamma * torch.max(self.target_model(next_state), dim=1)[0].unsqueeze(1)
            target_f = self.model(state)
            target_f[0][action] = target

            # 计算均方误差损失，并进行反向传播更新网络参数
            loss = nn.MSELoss()(target_f, self.model(state))
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def update_target_model(self):
        """
        将目标网络的参数更新为当前网络的参数，使目标网络跟上当前网络的变化，稳定训练过程。
        """
        self.target_model.load_state_dict(self.model.state_dict())


# 模拟已有的CNN-LSTM模型
input_size = 6
hidden_size = 30
num_layers = 3
output_size = 5
model = CNNLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)

# 模拟加载的模型参数
model.load_state_dict(torch.load(''.\weight\cnn_lstm_model_parameters_2024.12.6.pth'))
model.to(device)
model.eval()
print("Model parameters have been loaded successfully.")

# 创建环境
env = GasExtractionEnv(model)

# 获取状态和动作空间的维度
state_size = np.prod(env.observation_space.shape)  # 将4x6状态展平为一维
action_size = env.action_space.n  # 动作空间大小（243）

# 初始化DQN代理
agent = DQNAgent(state_size, action_size, QNetwork(state_size, 64, action_size))

# 用于存储每一轮训练的总奖励，方便后续可视化
episode_rewards = []

# 训练过程
episodes = 2000
for e in range(episodes):
    state = env.reset()
    state = state.flatten()  # 展平状态为一维向量
    total_reward = 0
    done = False
    while not done:
        # 选择动作
        action = agent.act(state)
        # 执行动作并得到反馈
        next_state, reward, done, info = env.step(action)
        next_state = next_state.flatten()  # 展平状态
        # 记住经验
        agent.remember(state, action, reward, next_state, done)
        # 更新状态
        state = next_state
        total_reward += reward
        # 训练
        agent.replay()
    # 每隔一定步数更新目标网络
    if e % 10 == 0:
        agent.update_target_model()
    episode_rewards.append(total_reward)
    print(f"Episode {e + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}")

# 绘制训练奖励曲线
plt.plot(range(1, episodes + 1), episode_rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Rewards over Episodes')
plt.show()

# 保存模型
torch.save(agent.model.state_dict(), "'.\weight\dqn_gas_extraction_model1.pth")


请帮我改进代码完成以下的需求：
画图的设置
1、多增加几个指标的图如：排水量变化、产气量变化等等
2、奖励的输出因为波动比较大，曲线加一个比较平均的图，比如一段区间内输出一个平均值
3、奖励的输出因为波动比较大，曲线加一个最优的图，比如一段区间内输出一个最大的奖励值

模型保存的设置
1、在训练的过程中保存最好的前五个模型

对奖励的设置
1、使用每次智能体改变了排水量和没改变排水量前的产气量的差来作为奖励，产气多则奖励多
2、每次改变的排水量，排水量多为惩罚，产气量增加为奖励，已知：生产1m3气的成本大概为0.01美元，生产1m3水的成本大概为10美元
3、每次改变的排水量，排水量增多为惩罚，产气量增加为奖励，改变了排水量也会给微小的惩罚


对动作的设置
1、5口井每口井3个动作243种
2、5口井一共3个动作，3种

额外思考：约束条件，如：压力越低越难抽水， pressure<0.5，则只能选择不抽水
对动作进行一个简单的设置，比如：
    1）如果当前排水量小于等于0.5，则只能选择不抽水
    2）如果当前排水量大于0.5，则可以选择不抽水，也可以选择抽水到0.5，也可以选择抽水到0.6，以此类推，直到抽水到最大值，即1.0
对动作进行一个简单的设置，比如：
    1）如果当前排水量小于等于0.5，则只能选择不抽


测试的代码
加载训练好的参数，在 GasExtractionEnv 环境下运行测试过程，
同时输出所做动作，原始状态，和改变后的状态。绘制出奖励曲线、产气量变化曲线、排水量变化曲线等，根据可视化结果直观分析模型在测试阶段的表现趋势，是否符合预期的稳定或者优化趋势。